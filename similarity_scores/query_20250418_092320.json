{
  "query": "What is the accuracy reward model?",
  "timestamp": "20250418_092320",
  "results": [
    {
      "text": "The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:\n\u00b7 Accuracy rewards : The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n\u00b7 Format rewards : In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between '<think>' and '</think>' tags.\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.",
      "similarity_score": 0.7479017183417613,
      "metadata": {
        "context": "- Details the reward system utilized for training DeepSeek-R1-Zero, crucial for optimizing RL processes.\n- Follows an explanation of the model's training methodology, building on previous sections about reinforcement learning.\n- Distinguishes between accuracy and format rewards, emphasizing the importance of structured responses.\n- Sets the stage for understanding the challenges faced during training, particularly regarding neural reward models and their limitations.",
        "filename": "DeepSeekR1.pdf",
        "page_numbers": [
          6
        ],
        "title": "2.2.2. Reward Modeling"
      }
    },
    {
      "text": "We employ three types of regularization during training:",
      "similarity_score": 0.7106452881581905,
      "metadata": {
        "context": "- Discusses training regularization methods used in Transformer model development.\n- Follows detailed training regime and performance metrics of the model.\n- Connects to overall model performance evaluation, emphasizing impact on translation quality.\n- Sets the stage for subsequent sections on results and comparisons with other architectures.",
        "filename": "AttentionIsAllYouNeed.pdf",
        "page_numbers": [
          7
        ],
        "title": "5.4 Regularization"
      }
    },
    {
      "text": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model's reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model's performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks.",
      "similarity_score": 0.7045894396201892,
      "metadata": {
        "context": "- Describes the application of large-scale reinforcement learning to enhance reasoning capabilities of DeepSeek-V3-Base after cold-start fine-tuning.\n- Addresses challenges encountered during training, particularly language mixing in chain-of-thought outputs.\n- Introduces a language consistency reward to improve readability and coherence of model outputs.\n- Connects to the document's focus on optimizing reasoning performance through innovative training methodologies and human-aligned reward structures.",
        "filename": "DeepSeekR1.pdf",
        "page_numbers": [
          10
        ],
        "title": "2.3.2. Reasoning-oriented Reinforcement Learning"
      }
    }
  ]
}