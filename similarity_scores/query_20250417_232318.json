{
  "query": "what is positional encoding?",
  "timestamp": "20250417_232318",
  "results": [
    {
      "text": "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\n\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2 \u03c0 to 10000 \u00b7 2 \u03c0 . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k , PE pos + k can be represented as a linear function of PE pos .\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.",
      "similarity_score": 0.6804762928429013,
      "metadata": {
        "context": "- Discusses the necessity of positional encodings to incorporate sequence order in the Transformer model.\n- Follows a description of the model's architecture, particularly addressing the absence of recurrence and convolution.\n- Connects to the main theme of enhancing model performance by allowing attention to focus on relative positions in sequences.\n- Prepares for empirical findings by comparing sinusoidal positional encodings and learned embeddings, referencing results from prior sections.",
        "filename": "AttentionIsAllYouNeed.pdf",
        "page_numbers": [
          6
        ],
        "title": "3.5 Positional Encoding"
      }
    },
    {
      "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.",
      "similarity_score": 0.6485696451013322,
      "metadata": {
        "context": "- Presents a comparative analysis of computational efficiency across various neural network architectures.\n- Follows a discussion on the importance of path lengths and operational complexity in sequence transduction tasks.\n- Supports the argument for the superiority of the Transformer's self-attention mechanism over recurrent and convolutional models.\n- Serves as a reference point for understanding the performance metrics related to model architecture choices throughout the document.",
        "filename": "AttentionIsAllYouNeed.pdf",
        "page_numbers": [
          6
        ],
        "title": "3.4 Embeddings and Softmax"
      }
    },
    {
      "text": "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.",
      "similarity_score": 0.644845459444334,
      "metadata": {
        "context": "- Describes the attention mechanism used in the Transformer architecture.\n- Elaborates on the computation of output from queries and keys, integral to the model's function.\n- Bridges theoretical concepts of attention with practical implementation within the model.\n- Sets up for further exploration of specific attention types, including Scaled Dot-Product Attention.",
        "filename": "AttentionIsAllYouNeed.pdf",
        "page_numbers": [
          4
        ],
        "title": "Scaled Dot-Product Attention"
      }
    }
  ]
}