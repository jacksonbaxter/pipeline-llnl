{
  "query": "positional encodings",
  "timestamp": "20250402_144820",
  "results": [
    {
      "text": "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\n\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2 \u03c0 to 10000 \u00b7 2 \u03c0 . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k , PE pos + k can be represented as a linear function of PE pos .\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.",
      "similarity_score": 0.5691967802557552,
      "metadata": {
        "context": "This chunk discusses the method of incorporating positional encodings into the Transformer model to account for the order of tokens in sequences, emphasizing the use of sine and cosine functions for encoding positions. It highlights the significance of positional information in the absence of recurrence and convolutions, and compares the effectiveness of sinusoidal encodings with learned positional embeddings, ultimately favoring the former for their potential to generalize to longer sequences.",
        "filename": "1706.03762v7.pdf",
        "page_numbers": [
          6
        ],
        "title": "3.5 Positional Encoding"
      }
    },
    {
      "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.",
      "similarity_score": 0.5335486079249115,
      "metadata": {
        "context": "This chunk presents Table 1, which summarizes key metrics such as maximum path lengths, per-layer complexity, and the minimum number of sequential operations associated with different layer types in neural network architectures, specifically in the context of comparing self-attention layers to recurrent and convolutional layers within the Transformer model discussed in the document.",
        "filename": "1706.03762v7.pdf",
        "page_numbers": [
          6
        ],
        "title": "3.4 Embeddings and Softmax"
      }
    },
    {
      "text": "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221a d model.",
      "similarity_score": 0.5319008951282544,
      "metadata": {
        "context": "This chunk describes the use of learned embeddings in the Transformer model for converting input and output tokens into vector representations, as well as the shared weight matrix between embedding layers and the linear transformation used for generating predicted probabilities in the decoder. It emphasizes the model's architecture and efficiency in handling token representations, which is central to the document's focus on the Transformer as an innovative sequence transduction model based on attention mechanisms.",
        "filename": "1706.03762v7.pdf",
        "page_numbers": [
          5
        ],
        "title": "3.4 Embeddings and Softmax"
      }
    }
  ]
}