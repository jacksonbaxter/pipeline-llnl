{
  "query": "how large is the stack of the decoder architecture?",
  "timestamp": "20250402_144337",
  "results": [
    {
      "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.",
      "similarity_score": 0.7181623866442193,
      "metadata": {
        "context": "This chunk presents Table 1, which summarizes key metrics such as maximum path lengths, per-layer complexity, and the minimum number of sequential operations associated with different layer types in neural network architectures, specifically in the context of comparing self-attention layers to recurrent and convolutional layers within the Transformer model discussed in the document.",
        "filename": "1706.03762v7.pdf",
        "page_numbers": [
          6
        ],
        "title": "3.4 Embeddings and Softmax"
      }
    },
    {
      "text": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).",
      "similarity_score": 0.6903172958686382,
      "metadata": {
        "context": "This chunk provides specific details about the training process of the models discussed in the document, including the hardware used (8 NVIDIA P100 GPUs), the duration and number of training steps for both base and big models, and the time per training step. This information is part of the broader exploration of the Transformer model's architecture and its performance on machine translation tasks.",
        "filename": "1706.03762v7.pdf",
        "page_numbers": [
          7
        ],
        "title": "5.2 Hardware and Schedule"
      }
    },
    {
      "text": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 0 . BLEU, establishing a new state-of-the-art BLEU score of 28 4 . . The configuration of this model is listed in the bottom line of Table 3. Training took 3 5 . days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 0 . , outperforming all of the previously published single models, at less than 1 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0 1 . , instead of 0 3 . .\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty \u03b1 = 0 6 . [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50 , but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .",
      "similarity_score": 0.6804957223474206,
      "metadata": {
        "context": "This chunk discusses the performance of the Transformer model on the WMT 2014 English-to-German and English-to-French translation tasks, highlighting its superior BLEU scores compared to previous models and its efficiency in training costs. It also details the training configuration, including dropout rates and hyperparameters used during model evaluation, as well as a reference to a summary table comparing translation quality and training costs across different models.",
        "filename": "1706.03762v7.pdf",
        "page_numbers": [
          8
        ],
        "title": "6.1 Machine Translation"
      }
    }
  ]
}