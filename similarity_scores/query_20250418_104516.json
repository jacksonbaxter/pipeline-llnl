{
  "query": "Why self attention?",
  "timestamp": "20250418_104516",
  "results": [
    {
      "text": "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.",
      "similarity_score": 0.7256193597170311,
      "metadata": {
        "context": "- Describes the attention mechanism used in the Transformer architecture.\n- Elaborates on the computation of output from queries and keys, integral to the model's function.\n- Bridges theoretical concepts of attention with practical implementation within the model.\n- Sets up for further exploration of specific attention types, including Scaled Dot-Product Attention.",
        "filename": "AttentionIsAllYouNeed.pdf",
        "page_numbers": [
          4
        ],
        "title": "Scaled Dot-Product Attention"
      }
    },
    {
      "text": "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum",
      "similarity_score": 0.7119596636870049,
      "metadata": {
        "context": "- Defines the attention mechanism, crucial for understanding the Transformer's architecture.\n- Follows a discussion on previous attention methods, contrasting them with the proposed approach.\n- Sets the stage for detailing the \"Scaled Dot-Product Attention,\" which is a key feature of the Transformer model.\n- Connects theoretical concepts to practical implementations within the model's framework.",
        "filename": "AttentionIsAllYouNeed.pdf",
        "page_numbers": [
          3
        ],
        "title": "3.2 Attention"
      }
    },
    {
      "text": "Self-Attention, Complexity per Layer = O n ( 2 \u00b7 d ). Self-Attention, Sequential Operations = O (1). Self-Attention, Maximum Path Length = O (1). Recurrent, Complexity per Layer = O n ( \u00b7 d 2 ). Recurrent, Sequential Operations = O n ( ). Recurrent, Maximum Path Length = O n ( ). Convolutional, Complexity per Layer = O k ( \u00b7 n \u00b7 d 2 ). Convolutional, Sequential Operations = O (1). Convolutional, Maximum Path Length = O log ( k ( n )). Self-Attention (restricted), Complexity per Layer = O r ( \u00b7 n \u00b7 d ). Self-Attention (restricted), Sequential Operations = O (1). Self-Attention (restricted), Maximum Path Length = O n/r ( )",
      "similarity_score": 0.6997230902252574,
      "metadata": {
        "context": "- Provides a comparative analysis of computational complexities for self-attention, recurrent, and convolutional layers, emphasizing the efficiency of self-attention.\n- Follows a discussion on the architectural advantages of the Transformer in handling sequence transduction tasks.\n- Sets the stage for deeper exploration of self-attention mechanics and their implications for model performance.\n- Connects to the document's main argument regarding the superiority of the Transformer model over traditional recurrent architectures in terms of parallelization and learning long-range dependencies.",
        "filename": "AttentionIsAllYouNeed.pdf",
        "page_numbers": [
          6
        ],
        "title": "3.4 Embeddings and Softmax"
      }
    }
  ]
}